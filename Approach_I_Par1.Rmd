---
title: "Approach I"
author: "Astha Bista"
date: "2/9/2021"
output: md_document
bibliography: [packages.bib, ref.bib]
biblio-style: "apalike"
link-citations: true
---
This document consists of two main parts of Approach I: Principal component analysis and cluster analysis. The data used were first made stationary by using first degree differencing.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r  warning=FALSE,, include=FALSE, echo=FALSE}
setwd("C:/Users/Aastha/Desktop/GWProject")
list.of.packages <- c("tseries","FactoMineR","factoextra","rela","psych","corrplot","NbClust",
                      "kableExtra","dplyr","tidyr","reshape2","ggplot2")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library("FactoMineR")
library("factoextra")
library(rela)
library(psych)
library(corrplot)
library(tseries)
library(NbClust)
library(dplyr)
library(tidyr)
library(kableExtra)
library(reshape2)
library(ggplot2)
```
Before beginning, here is a function to plot many time series using ggplot.We will be using this function in the following lines to plot stationary time series.

```{r}
create_timeseries_plots <- function(df){
  # change into metres
  df <- sapply(as.data.frame(df), function(y) y*0.3048)
  # column with dates
  dates = seq(from = as.Date("2000-02-01"), to = as.Date("2018-12-1"), by = 'month') 
  # add date column to dataframe
  df_m <- data.frame(as.data.frame(df[,-1]),dates)
  colnames(df_m)[ncol(df_m)]<-"Date"
  # Rearrange dataframe to long form
  df_m2 <- melt(df_m, id.vars = "Date", 
                variable.name = "Var", value.name="Val")
  # Group the dataframe
  df_m3 <- df_m2%>%
    group_by(Date,Var)
  #Plot using ggplot2
  ggplot(data = df_m3,aes(x = Date, y = Val)) + 
    geom_line() + facet_wrap(~ Var, nrow = 6, scales = "free") +
    xlab("Year") + ylab("Units(m)") + theme_bw()
}

```


# Converting time series to stationary
In this approach, the time series data is made stationary using first differencing methods. For testing stationarity, Augmented Dickey-Fuller (ADF) t-statistic test for unit root was used.

```{r warning=FALSE}
GWData<-read.csv(file ="Data_Processing/GWLevel_imputed.csv",header = TRUE)  
head(GWData)
```
Now, loop around each column in the dataframe to obtain a dataframe containing first degree differenced values.

```{r warning=FALSE,include=FALSE}
diff_gw<-c()
for(i in 2:13){               #Loop around 13 wells groundwater levels
  diff_dat<-diff(GWData[,i])    #First degree differencing
  diff_gw<-cbind(diff_gw,diff_dat)
}

colnames(diff_gw)<-c("W60","W63","W67","W70","W73","W74","W78","W80","W81","W115","W116","W118")

# Check stationarity of each column in new dataframe using Augmented Dickey-Fuller (ADF) 
# t-statistic test for unit root. If H0 is rejected, the series is stationary
for(i in 1:12){ 
  print(adf.test(diff_gw[,i])$p.value)  #Alternate hypotheisis: stationary
}
```
This above procedure were also used to create stationary time series of stream stage, precipitation, and pumping.
Below is the plot showing the stationary time series. Please note that the units are differences of water levels between a month and preceding month, therefore the negative values in some cases.
```{r  fig.height=10,fig.width=10, warning=FALSE}
create_timeseries_plots(diff_gw)
```

# Principal Component Analysis

### Check suitability of PCA
```{r }
#First compute correlation matrix
GWDataCor <- cor(GWData)
```

We now examine the data to assess whether the assumptions for PCA have been met before proceeding. For this, instructions from [@field2012discovering] were used. We use the paf() function from the rela package [@R-rela].
```{r  include=FALSE}
assumptions <- paf(as.matrix(GWData), eigcrit = 1, convcrit = .001)
```

To test for the first assumption, Bartlett’s Test for Sphericity was performed. The null hypothesis for this test is that the intercorrelation matrix comes from a noncollinear populaton or simply that there is nocollinearity between the variables, which would render PCA impossible as it depends on the construction of a linear combination of the variables. We use a significance level α=.05.
```{r ,warning=FALSE}
bartlettTest <- cortest.bartlett(GWDataCor)
bartlettTest
```

Bartlett test is rejected because p values is less than 0.01. Now we will extract Kaiser-Meyer-Olkin (KMO) from the assumptions object.
```{r }
print(assumptions$KMO)
```

This value is more than 0.7, so can be acceptable. 
```{r}
det(GWDataCor)
```
The determinant is positive, hence it satisfies all three assumptions of PCA. Therefore, we can proceed with PCA.

### Analyzing correlation matrix
The correlation matrix is shown graphically. This following plot gives a prelimminary insight on the relationship between the groundwater levels in wells.
```{r}
cor.mat <- round(cor(GWData),2)
corrplot(cor.mat, type="upper", order="hclust", 
         tl.col="black", tl.srt=45)
```

### Performing PCA
PCA analysis operation was followed step by step from [@kassambara2017practical] and [link](http://www.sthda.com/english/wiki/print.php?id=202). The data is first standardized by centering and scaling.
```{r}
GWData<-scale(GWData)
```
Now, FactoMine package [@FactoMineR2008] is used to perform PCA.
```{r}
res.pca <- PCA(GWData, graph = F)
```
```{r echo=FALSE}
eigenvalues <- res.pca$eig
barplot(eigenvalues[, 2], names.arg=1:nrow(eigenvalues), 
        main = "Variances",
        xlab = "Principal Components",
        ylab = "Percentage of variances",
        col ="steelblue")
lines(x = 1:nrow(eigenvalues), eigenvalues[, 2], 
      type="b", pch=19, col = "red")
```

The above screeplot shows that three components are dominant in the data. Therefore, we will use three principal components. The variables can also be shown in a base graph showing contribution levels of each well along with information on the alignment of each well on principal components.
```{r }
fviz_pca_var(res.pca, col.var="contrib")+ 
  scale_color_gradient2(low="white",mid="blue",high="red", midpoint=2.5)+theme_bw()
```

### PCA results
The main output of PCA are PC scores and PC loadings.
```{r }
loadings<-sweep(res.pca$var$coord,2,sqrt(res.pca$eig[1:ncol(res.pca$var$coord),1]),FUN="/")
scores <- res.pca$ind$coord
```
Loadings:
```{r }
head(loadings)
```
Scores:
```{r }
head(scores)
```

# Cluster Analysis
Loadings from PCA was used in cluster analysis.
```{r}
ldfile<-read.csv(file = "Approach_I/PC_loadings.csv",header = T)
selectedData<-ldfile[,2:4]
row.names(selectedData)<-c("W60","W63","W67","W70","W73","W74","W78","W80","W81","W115",
                           "W116","W118")
selectedData
```

Elbow method to find optimal number of clusters:
```{r fig.height=3,fig.width=4}
fviz_nbclust(selectedData, kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow method")
```

Using heirarchical clustering:
```{r fig.height=5,fig.width=7}
distanceData = dist(selectedData, method="euclidean")
hclustData<-hclust(distanceData,method="ward.D")
plot(hclustData,main=" ",sub="Heirarchical clustering",xlab=" ")
rect.hclust(hclustData, k=3,border = "red")
mtext('Group A',side = 1,at = 3)
mtext('Group B',side = 1,at = 7)
mtext('Group C',side = 1,at = 10)
```

Using k-means clustering:
```{r}
kmeansData<-kmeans(selectedData,3) 
ClusterData<-kmeansData$cluster
```

Now, plotting loadings of PC1 against P2, PC2 against PC3, and PC3 against PC1
```{r fig.height=5,fig.width=6}
palette(c("maroon","dark green","blue"))
plot(selectedData[,1],selectedData[,2],ylim=c(-0.3,0.6),xlim=c(-0.15,0.45),
     xlab="PC1",ylab="PC2",pch=19,cex=1,lty='solid',lwd=2,col= ClusterData)
text(selectedData[,c(1,2)],labels=rownames(selectedData),cex=1,pos=3,col= ClusterData)
legend('topright',legend=c("Group A","Group B","Group C"),col=c("maroon","blue","dark green"),pch=19,
       cex=1,bty="y")
grid()
```

```{r fig.height=5,fig.width=6}
plot(selectedData[,2],selectedData[,3],ylim=c(-0.5,0.7),xlim=c(-0.3,0.6),
     xlab="PC2",ylab="PC3",pch=19,cex=1,lty='solid',lwd=2,col= ClusterData)
text(selectedData[,c(2,3)],labels=rownames(selectedData),cex=1,pos=3,col= ClusterData)
legend('topright',legend=c("Group A","Group B","Group C"),col=c("maroon","blue","dark green"),pch=19,
       cex=1,bty="y")
```

```{r fig.height=5,fig.width=6}
plot(selectedData[,1],selectedData[,3],ylim=c(-0.5,0.6),xlim=c(-0.1,0.5),
     xlab="PC1",ylab="PC3",pch=19,cex=1,lty='solid',lwd=2,col= ClusterData)
text(selectedData[,c(1,3)],labels=rownames(selectedData),cex=1,pos=3,col= ClusterData)
legend('topright',legend=c("Group A","Group B","Group C"),col=c("red","blue","dark green"),pch=19,
       cex=1,bty="y")
```

# Comparison of principal components and groundwater wells
The principal components can be compared with the groundwater wells to validate the cluster analysis above. First, all the variables need to be transformed into time series form.
```{r}
Scores <- read.csv(file = "Approach_I/PC_scores.csv",header = T)[-1]
W <- scale(read.csv("Approach_I/GWLevels_stationary.csv", sep=","))
```

```{r}
#PC time series
pc1.ts<-ts(Scores[,1],start=c(2000,1),end=c(2018,12),frequency = 12)
pc2.ts<-ts(Scores[,2],start=c(2000,1),end=c(2018,12),frequency = 12)
pc3.ts<-ts(Scores[,3],start=c(2000,1),end=c(2018,12),frequency = 12)

#Wells time series
w60.ts<-ts(W[,2],start=c(2000,1),end=c(2018,12),frequency = 12)
w63.ts<-ts(W[,3],start=c(2000,1),end=c(2018,12),frequency = 12) 
w67.ts<-ts(W[,4],start=c(2000,1),end=c(2018,12),frequency = 12) 
w70.ts<-ts(W[,5],start=c(2000,1),end=c(2018,12),frequency = 12) 
w73.ts<-ts(W[,6],start=c(2000,1),end=c(2018,12),frequency = 12) 
w74.ts<-ts(W[,7],start=c(2000,1),end=c(2018,12),frequency = 12) 
w78.ts<-ts(W[,8],start=c(2000,1),end=c(2018,12),frequency = 12) 
w80.ts<-ts(W[,9],start=c(2000,1),end=c(2018,12),frequency = 12) 
w81.ts<-ts(W[,10],start=c(2000,1),end=c(2018,12),frequency = 12) 
w115.ts<-ts(W[,11],start=c(2000,1),end=c(2018,12),frequency = 12) 
w116.ts<-ts(W[,12],start=c(2000,1),end=c(2018,12),frequency = 12) 
w118.ts<-ts(W[,13],start=c(2000,1),end=c(2018,12),frequency = 12) 

```
Here is a plot showing the PC scores:
```{r}
#Plot PC scores
par(mfrow=c(3,1), mar=c(0,3.5,0,1), oma=c(4,0,2,0), mgp=c(2,.6,0), cex.lab=1.1, tcl=-.3, las=1)
#Group1
ts.plot(pc1.ts,
        gpars = list(xlab="Year",ylab="Score",cex=1,xaxt="n"),ylim=c(-7,7),
        col=c("black"),lwd=c(1,2,2,2),lty=c(1,1,1,1,1,1))
abline(v=seq(2000,2019,1),lty = 6, col = "cornsilk2")
#abline(v=seq(2000,2019,1/12),lty = 3,lwd=0.01, col = "cornsilk2")
grid (NA,NULL, lty = 6, col = "cornsilk2")
text(2003,6.5,"PC1: 35.75% variance explained",cex=1.5,bty="n")

#Group2
ts.plot(pc2.ts,
        gpars = list(xlab="Year",ylab="Score",cex=1,xaxt="n"),ylim=c(-7,7),
        col=c("black"),lwd=c(1,2,2,2),lty=c(1,1,1,1,1,1))
abline(v=seq(2000,2019,1),lty = 6, col = "cornsilk2")
#abline(v=seq(2000,2019,1/12),lty = 3,lwd=0.01, col = "cornsilk2")
grid (NA,NULL, lty = 6, col = "cornsilk2")
text(2003,6.6,"PC2: 17.30% variance explained",cex=1.5,bty="n")

#Group3
ts.plot(pc3.ts,
        gpars = list(xlab="Year",ylab="Score",cex=1,xaxt="n"),ylim=c(-7,7),
        col=c("black"),lwd=c(1,2,2,2),lty=c(1,1,1,1,1,1))
abline(v=seq(2000,2019,1),lty = 6, col = "cornsilk2")
#abline(v=seq(2000,2019,1/12),lty = 3,lwd=0.01, col = "cornsilk2")
grid (NA,NULL, lty = 6, col = "cornsilk2")
text(2003,6.6,"PC3: 16.32% variance explained",cex=1.5,bty="n")

year.text=c("2000","2001","2002","2003","2004","2005","2006","2007","2008","2009","2010","2011","2012","2013",
            "2014","2015","2016","2017","2018","2019")
axis(1,at=seq(2000,2019,1),labels=year.text)
title(xlab="Year", outer=TRUE)
```
Now, the wells having correlations with each PC are plotted with the respective PC.
```{r ,fig.height=7,fig.width=9}
par(mfrow=c(3,1), mar=c(0,3.5,0,3), oma=c(4,2,2,0), mgp=c(2,.6,0), cex.lab=1.5, tcl=-.3, las=1)

#PC1
ts.plot(pc1.ts,w63.ts,w81.ts,w78.ts,
        gpars = list(xlab=" ",ylab=" ",cex=1,xaxt="n",yaxt ="n"),ylim=c(-6,6),
        col=c("red","gray","gold4","darkseagreen"),lwd=c(2,2,2,2),lty=c(1,1,1,1))
axis(2,cex.axis=1.2)

abline(v=seq(2000,2019,1),lty = 6, col = "cornsilk2")
grid (NA,NULL, lty = 6, col = "cornsilk2")
legend('topleft',horiz = TRUE,legend=c("PC1","W63","W81","W78"),
       col=c("red","gray","gold4","darkseagreen"),lwd=c(2,2,2,2),lty=c(1,1,1,1),cex=1.5,bty="y")
text(2016,5.5,"PC1 with Group A wells",cex=1.5,bty="n",font=1)



#PC2
ts.plot(pc2.ts,w60.ts,w74.ts,w80.ts,
        gpars = list(xlab=" ",ylab=" ",xaxt="n",yaxt ="n"),ylim=c(-6,6),
        col=c("red","gray","gold4","darkseagreen"),lwd=c(2,2,2,2),lty=c(1,1,1,1))

abline(v=seq(2000,2019,1),lty = 6, col = "cornsilk2")
grid (NA,NULL, lty = 6, col = "cornsilk2")
legend('topleft',horiz = TRUE,legend=c("PC2","W60","W74","W80"),
       col=c("red","gray","gold4","darkseagreen"),lwd=c(2,2,2,2),lty=c(1,1,1,1),cex=1.5,bty="y")
text(2016,5.5,"PC2 with Group B wells",cex=1.5,bty="n",font=1)
axis(2,cex.axis=1.2)

#PC3
ts.plot(pc3.ts,w70.ts,w73.ts,w115.ts,
        gpars = list(xlab=" ",ylab=" ",cex=1,xaxt="n",yaxt ="n"),ylim=c(-6,6),
        col=c("red","gray","gold4","darkseagreen"),lwd=c(2,2,2,2),lty=c(1,1,1,1))

abline(v=seq(2000,2019,1),lty = 6, col = "cornsilk2")
grid (NA,NULL, lty = 6, col = "cornsilk2")
legend('topleft',horiz = TRUE,legend=c("PC3","W70","W73","W115"),
       col=c("red","gray","gold4","darkseagreen"),lwd=c(2,2,2,2),lty=c(1,1,1,1),cex=1.5,bty="y")
text(2016,5.5,"PC3 with Group C wells",cex=1.5,bty="n",font=1)
axis(2,cex.axis=1.2)

year.text=c("2000","2001","2002","2003","2004","2005","2006","2007","2008","2009","2010","2011","2012","2013",
            "2014","2015","2016","2017","2018","2019")
axis(1,at=seq(2000,2019,1),labels=year.text,cex.axis = 1.2)

mtext("Standardized units",side = 2,outer = TRUE,las = 0,cex = 1)
mtext("Year",side = 1,outer = TRUE,line = 2,cex = 1)

```
# Cross correlation of variables
For calculating cross correlation, ccf() from tseries package [@R-tseries] was used. This function computes the sample crosscorrelation (covariance) function of x and y up to lag lag. If pl is TRUE, then the crosscorrelation (covariance) function is plotted. For the crosscorrelation function also the 95% confidence bounds for strict white noise are plotted. Uses fft for efficiency reasons.

Preparing the data:
```{r }
W <- read.csv("Approach_I/GWLevels_stationary.csv", sep=",")[-1]
S <- read.csv("Approach_I/SW_stationary.csv", sep=",")[-1]
P <- read.csv("Approach_I/Precipitation_stationary.csv", sep=",")[-1]
Pu <- -read.csv("Data_Processing/Pumping.csv", sep=",")[-1]
```
```{r}
#Combining all variables together
All <- data.frame(W,S,P)
All$Pu <- c(unlist(Pu),rep(NA, nrow(All)-nrow(Pu)))
tbl_df(All)
round(cor(All, use="complete.obs"),2)
```

Here, first two functions were created
* FUnction to find maximum correlation
```{r}
Find_Max_CCF<- function(a,b)
{
  d <- ccf(a, b, plot = FALSE,na.action = na.contiguous)
  cor = d$acf[,,1]
  lag = d$lag[,,1]
  res = data.frame(cor,lag)
  res_max = res[which.max(res$cor),]
  return(res_max)
} 
```

* Function to determine crosscorrelations for each pair of variables
```{r}
Create_crosscorrelation_table<- function(df)
{
  col<-colnames(df)
  addData<-as.data.frame(matrix(NA, ncol = 2, nrow = (ncol(All))^2 - ncol(All)))
  varnames<-data.frame()
  rownumber=1
  #Loop for selecting the first variable
  for(i in 1:ncol(df)){
    #Build time series of first variable
    a.ts<-ts(df[,i],start=c(2000,1),end=c(2018,12),frequency = 12) 
    #Loop around for selecting the second variable
    for(j in 1:ncol(df)){
      if(i!=j){             #removing selecting same variable  
        #build time series of second variable
        b.ts<-ts(df[,j],start=c(2000,1),end=c(2018,12),frequency = 12)
        #use given function to find the maximum correlation and its lag
        ccfmax<-Find_Max_CCF(a.ts,b.ts)
        addData[rownumber,1]<-ccfmax[1]  #add correlation value to first column
        addData[rownumber,2]<-ccfmax[2]  #add lag to second column
        varnames[rownumber,1]<-col[i]    #add first variable name to first column
        varnames[rownumber,2]<-col[j]    #add second variable name to second column
        rownumber<-rownumber+1   
      }
    }
  }
  new_df<-data.frame(varnames,addData)   #Combine names dataframe and values dataframe
  new_df$Days<-new_df[,4]*12             #Convert lag to months
  colnames(new_df)<-c("Var1","Var2","Cor","Lag","Months")
  return(new_df)
} 
```
Now, using the above function in our data.
```{r}
new_df <- Create_crosscorrelation_table(All)
```

Now, we need to arrange this table in a presentable form.
```{r}
## Arrange table in a horizontal form
tbl_df(new_df)
new_df<- new_df[,c(1,2,3,5)]
colnames(new_df)<- c("X","Y","Cor","Months")

df2 <- new_df %>%
  filter(grepl("W",X)) %>%        #select only wells in first column
  filter(!grepl("W",Y)) %>%       #select other variables in second column
  gather(Var, val, c(Cor, Months)) %>%   #arrange dataframe in a long form
  unite(Var1,Var, Y) %>%                 #unite headings: e.g.Cor_G40,Months_G40
  pivot_wider(names_from = Var1, values_from = val)   #arrange dataframe in a wide form

#Seoarate columns with correlation and months
cor_df<-cbind(df2[,1],round(df2[,2:18],3))
mth_df<-df2[,19:35]
#arrange the order
neworder <- order(c(2*(seq_along(cor_df[,-1]) - 1) + 1,
                    2*seq_along(mth_df)))
df3<-cbind(Wells = cor_df[,1],cbind(cor_df[,-1], mth_df)[,neworder])
colnames(df3)[1] <- "Wells"

#Group the wells into three groups
df4<-df3 %>%
  mutate(Group = ifelse(grepl("W63|W67|W78|W81|W116",Wells),"Group A",
                               ifelse(grepl("W60|W74|W80",Wells),"Group B",
                                      ifelse(grepl("W70|W73|W115|W118",Wells),"Group C","None")))) %>%
  select(Wells, Group, Cor_G40:Months_Pu) %>%
  group_by(Group) %>%
  arrange(Group)
df4
```


# References
